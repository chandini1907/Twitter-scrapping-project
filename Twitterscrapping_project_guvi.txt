#import modules
import snscrape.modules.twitter as sntwitter
import pandas as pd
import streamlit as st
import datetime
import pymongo
from pymongo import MongoClient
import time

#make a connection with MongoDB
client = pymongo.MongoClient("mongodb://localhost:27017/")  
my_coll = client["Twitter_collection1"]

#title of web page
st.title("Twitter Scrapper")
genre=st.radio("Enter Your",('Keyword','Hashtag'))
word=st.text_input("What you want to search?")
start=st.date_input("Enter your start date(YYYY/MM/DD)",datetime.date(2022,1,1))
end=st.date_input("Enter your end date(YYYY/MM/DD)",datetime.date(2023,1,1))
limit= st.slider('Select a Limit:', 0, 1000, 10)

# Creating list to append tweet data to
tweets_list2 = []


# Using TwitterSearchScraper to scrape data and append tweets to list

if genre=='Keyword':
    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{word} + since:{start} until:{end}').get_items()):
        if i>limit:
            break
        tweets_list2.append([ tweet.id, tweet.date,  tweet.content, tweet.lang, tweet.user.username, tweet.replyCount, tweet.retweetCount,tweet.likeCount, tweet.source, tweet.url ])
    df_tweets = pd.DataFrame(tweets_list2, columns=['ID','Date','Content', 'Language', 'Username', 'ReplyCount', 'RetweetCount', 'LikeCount','Source', 'Url'])
else:
    for i,tweet in enumerate(sntwitter.TwitterHashtagScraper(f'{word} + since:{start} until:{end}').get_items()):
        if i>limit:
            break            
        tweets_list2.append([ tweet.id, tweet.date,  tweet.content, tweet.lang, tweet.user.username, tweet.replyCount, tweet.retweetCount,tweet.likeCount, tweet.source, tweet.url ])
    df_tweets = pd.DataFrame(tweets_list2, columns=['ID','Date','Content', 'Language', 'Username', 'ReplyCount', 'RetweetCount', 'LikeCount','Source', 'Url'])



 # UPLOAD DATA TO MongoDB

if st.button('Upload to MongoDB'):
    coll=word
    coll=coll.replace(' ','_')+'_Tweets'
    mycoll=my_coll[coll]
    dict=df_tweets.to_dict('records')
    if dict:
        mycoll.insert_many(dict) 
        ts = time.time()
        mycoll.update_many({}, {"$set": {"KeyWord_or_Hashtag": word+str(ts)}}, upsert=False, array_filters=None)
        st.success('Successfully uploaded to database', icon="✅")
    else:
        st.warning('Cant upload because there are no tweets', icon="⚠️")

#Download csv file
@st.cache
def convert_df(df):
    return df.to_csv().encode('utf-8')
if not df_tweets.empty:
    csv=convert_df(df_tweets)
    st.download_button(label="Download csv file",data=csv,file_name='Twitter_df.csv',mime='text/csv',)

#download json file
    json_string = df_tweets.to_json(orient ='records')
    st.download_button(label="Download Json File",file_name="Twitter_df.json",mime="application/json",data=json_string,)


# Creating a dataframe from the tweets list above
if st.button("Your Dataframe"):
    st.dataframe(df_tweets)
